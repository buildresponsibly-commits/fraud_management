{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c7dc9-17d4-47fd-aa9d-9f00fc53601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Optimization Strategies for Fraud Detection\n",
    "# Business Constraints: Decline rate â‰¤ 30%, Agent alerts < 0.1%, Missed fraud â‰¤ 0.02%\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"MODEL OPTIMIZATION STRATEGIES FOR FRAUD DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 1: ADVANCED FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def advanced_feature_engineering(df):\n",
    "    \"\"\"Advanced feature engineering techniques\"\"\"\n",
    "    \n",
    "    print(\"\\n1. ADVANCED FEATURE ENGINEERING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1.1 Polynomial features for key variables\n",
    "    print(\"Creating polynomial features...\")\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    # Apply to most discriminative features only\n",
    "    key_features = ['V4', 'V11', 'V12', 'V14', 'Amount']  # Example key features\n",
    "    if all(col in df.columns for col in key_features):\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "        poly_features = poly.fit_transform(df[key_features])\n",
    "        poly_feature_names = [f\"poly_{i}\" for i in range(poly_features.shape[1] - len(key_features))]\n",
    "        \n",
    "        for i, name in enumerate(poly_feature_names):\n",
    "            df_enhanced[name] = poly_features[:, len(key_features) + i]\n",
    "    \n",
    "    # 1.2 Time-based feature engineering\n",
    "    print(\"Creating advanced time features...\")\n",
    "    if 'Time' in df.columns:\n",
    "        df_enhanced['Hour'] = (df['Time'] % (24*3600)) // 3600\n",
    "        df_enhanced['Minute'] = (df['Time'] % 3600) // 60\n",
    "        df_enhanced['Day_of_transaction'] = df['Time'] // (24*3600)\n",
    "        \n",
    "        # Cyclic encoding for time\n",
    "        df_enhanced['Hour_sin'] = np.sin(2 * np.pi * df_enhanced['Hour'] / 24)\n",
    "        df_enhanced['Hour_cos'] = np.cos(2 * np.pi * df_enhanced['Hour'] / 24)\n",
    "        df_enhanced['Minute_sin'] = np.sin(2 * np.pi * df_enhanced['Minute'] / 60)\n",
    "        df_enhanced['Minute_cos'] = np.cos(2 * np.pi * df_enhanced['Minute'] / 60)\n",
    "        \n",
    "        # Time-based patterns\n",
    "        df_enhanced['Is_Night'] = ((df_enhanced['Hour'] >= 23) | (df_enhanced['Hour'] <= 6)).astype(int)\n",
    "        df_enhanced['Is_Business_Hour'] = ((df_enhanced['Hour'] >= 9) & (df_enhanced['Hour'] <= 17)).astype(int)\n",
    "        df_enhanced['Is_Weekend'] = ((df_enhanced['Day_of_transaction'] % 7) >= 5).astype(int)\n",
    "    \n",
    "    # 1.3 Amount-based feature engineering\n",
    "    print(\"Creating advanced amount features...\")\n",
    "    if 'Amount' in df.columns:\n",
    "        df_enhanced['Amount_log'] = np.log1p(df['Amount'])\n",
    "        df_enhanced['Amount_sqrt'] = np.sqrt(df['Amount'])\n",
    "        df_enhanced['Amount_cbrt'] = np.cbrt(df['Amount'])\n",
    "        \n",
    "        # Amount percentile ranking\n",
    "        df_enhanced['Amount_percentile'] = df['Amount'].rank(pct=True)\n",
    "        \n",
    "        # Amount categories with business logic\n",
    "        df_enhanced['Amount_micro'] = (df['Amount'] <= 1).astype(int)\n",
    "        df_enhanced['Amount_small'] = ((df['Amount'] > 1) & (df['Amount'] <= 100)).astype(int)\n",
    "        df_enhanced['Amount_medium'] = ((df['Amount'] > 100) & (df['Amount'] <= 1000)).astype(int)\n",
    "        df_enhanced['Amount_large'] = (df['Amount'] > 1000).astype(int)\n",
    "    \n",
    "    # 1.4 PCA feature combinations and statistics\n",
    "    print(\"Creating PCA feature combinations...\")\n",
    "    v_columns = [col for col in df.columns if col.startswith('V')]\n",
    "    if len(v_columns) >= 10:\n",
    "        df_enhanced['V_sum'] = df[v_columns].sum(axis=1)\n",
    "        df_enhanced['V_mean'] = df[v_columns].mean(axis=1)\n",
    "        df_enhanced['V_std'] = df[v_columns].std(axis=1)\n",
    "        df_enhanced['V_skew'] = df[v_columns].skew(axis=1)\n",
    "        df_enhanced['V_kurt'] = df[v_columns].kurtosis(axis=1)\n",
    "        df_enhanced['V_median'] = df[v_columns].median(axis=1)\n",
    "        df_enhanced['V_max'] = df[v_columns].max(axis=1)\n",
    "        df_enhanced['V_min'] = df[v_columns].min(axis=1)\n",
    "        df_enhanced['V_range'] = df_enhanced['V_max'] - df_enhanced['V_min']\n",
    "        \n",
    "        # Count positive and negative values\n",
    "        df_enhanced['V_positive_count'] = (df[v_columns] > 0).sum(axis=1)\n",
    "        df_enhanced['V_negative_count'] = (df[v_columns] < 0).sum(axis=1)\n",
    "        df_enhanced['V_zero_count'] = (df[v_columns] == 0).sum(axis=1)\n",
    "    \n",
    "    print(f\"Features increased from {df.shape[1]} to {df_enhanced.shape[1]}\")\n",
    "    return df_enhanced\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 2: ADVANCED SAMPLING TECHNIQUES\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_sampling_strategy(X_train, y_train):\n",
    "    \"\"\"Test multiple sampling strategies to handle class imbalance\"\"\"\n",
    "    \n",
    "    print(\"\\n2. ADVANCED SAMPLING OPTIMIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    sampling_strategies = {\n",
    "        'SMOTE': SMOTE(random_state=42, k_neighbors=3),\n",
    "        'ADASYN': ADASYN(random_state=42, n_neighbors=3),\n",
    "        'BorderlineSMOTE': BorderlineSMOTE(random_state=42, k_neighbors=3),\n",
    "        'SMOTE_Tomek': SMOTETomek(random_state=42),\n",
    "        'SMOTE_ENN': SMOTEENN(random_state=42),\n",
    "    }\n",
    "    \n",
    "    sampling_results = {}\n",
    "    \n",
    "    for name, sampler in sampling_strategies.items():\n",
    "        try:\n",
    "            print(f\"Testing {name}...\")\n",
    "            X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "            \n",
    "            # Quick evaluation with simple model\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            rf_temp = RandomForestClassifier(n_estimators=50, random_state=42, class_weight='balanced')\n",
    "            rf_temp.fit(X_resampled, y_resampled)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_score = rf_temp.score(X_train, y_train)  # Using original training set as validation\n",
    "            \n",
    "            sampling_results[name] = {\n",
    "                'original_size': len(X_train),\n",
    "                'resampled_size': len(X_resampled),\n",
    "                'original_fraud_rate': y_train.mean(),\n",
    "                'resampled_fraud_rate': y_resampled.mean(),\n",
    "                'validation_accuracy': val_score\n",
    "            }\n",
    "            \n",
    "            print(f\"  Original size: {len(X_train):,} â†’ Resampled: {len(X_resampled):,}\")\n",
    "            print(f\"  Fraud rate: {y_train.mean():.4f} â†’ {y_resampled.mean():.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {name}: {str(e)}\")\n",
    "    \n",
    "    # Find best sampling strategy\n",
    "    best_sampler = max(sampling_results.keys(), \n",
    "                      key=lambda k: sampling_results[k]['validation_accuracy'])\n",
    "    \n",
    "    print(f\"\\nBest sampling strategy: {best_sampler}\")\n",
    "    print(f\"Validation accuracy: {sampling_results[best_sampler]['validation_accuracy']:.4f}\")\n",
    "    \n",
    "    return sampling_strategies[best_sampler], sampling_results\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 3: HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_model_hyperparameters(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Comprehensive hyperparameter optimization\"\"\"\n",
    "    \n",
    "    print(\"\\n3. HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Custom scorer for business constraints\n",
    "    def business_scorer(y_true, y_pred_proba, decline_weight=0.4, alert_weight=0.4, fraud_weight=0.2):\n",
    "        \"\"\"Custom scorer considering business constraints\"\"\"\n",
    "        # This is a simplified version - implement based on your threshold logic\n",
    "        return roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Model configurations to test\n",
    "    model_configs = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300, 500],\n",
    "                'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'class_weight': ['balanced', 'balanced_subsample'],\n",
    "                'bootstrap': [True, False]\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'XGBoost': {\n",
    "            'model': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'scale_pos_weight': [1, 10, 50, 100]\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'LightGBM': {\n",
    "            'model': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'num_leaves': [31, 50, 100],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'class_weight': ['balanced']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_models = {}\n",
    "    \n",
    "    for model_name, config in model_configs.items():\n",
    "        print(f\"\\nOptimizing {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Use RandomizedSearchCV for faster optimization\n",
    "            random_search = RandomizedSearchCV(\n",
    "                config['model'],\n",
    "                config['params'],\n",
    "                n_iter=20,  # Reduced for faster execution\n",
    "                cv=StratifiedKFold(n_splits=3),\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=-1,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            random_search.fit(X_train, y_train)\n",
    "            \n",
    "            best_models[model_name] = {\n",
    "                'model': random_search.best_estimator_,\n",
    "                'params': random_search.best_params_,\n",
    "                'cv_score': random_search.best_score_,\n",
    "                'val_score': random_search.best_estimator_.score(X_val, y_val)\n",
    "            }\n",
    "            \n",
    "            print(f\"  Best CV Score: {random_search.best_score_:.4f}\")\n",
    "            print(f\"  Validation Score: {best_models[model_name]['val_score']:.4f}\")\n",
    "            print(f\"  Best Params: {random_search.best_params_}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error optimizing {model_name}: {str(e)}\")\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 4: ENSEMBLE OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_optimized_ensemble(best_models, X_train, y_train):\n",
    "    \"\"\"Create optimized ensemble with dynamic weighting\"\"\"\n",
    "    \n",
    "    print(\"\\n4. ENSEMBLE OPTIMIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Extract trained models\n",
    "    models_list = [(name, model_info['model']) for name, model_info in best_models.items()]\n",
    "    \n",
    "    # Test different ensemble strategies\n",
    "    ensemble_strategies = {}\n",
    "    \n",
    "    # 1. Voting Classifier with different weights\n",
    "    if len(models_list) >= 2:\n",
    "        print(\"Testing Voting Classifier...\")\n",
    "        \n",
    "        # Equal weights\n",
    "        voting_equal = VotingClassifier(\n",
    "            estimators=models_list,\n",
    "            voting='soft'\n",
    "        )\n",
    "        voting_equal.fit(X_train, y_train)\n",
    "        ensemble_strategies['voting_equal'] = voting_equal\n",
    "        \n",
    "        # Performance-based weights\n",
    "        weights = [best_models[name]['cv_score'] for name, _ in models_list]\n",
    "        voting_weighted = VotingClassifier(\n",
    "            estimators=models_list,\n",
    "            voting='soft',\n",
    "            weights=weights\n",
    "        )\n",
    "        voting_weighted.fit(X_train, y_train)\n",
    "        ensemble_strategies['voting_weighted'] = voting_weighted\n",
    "    \n",
    "    # 2. Custom ensemble with business logic\n",
    "    class BusinessOptimizedEnsemble:\n",
    "        def __init__(self, models, weights=None):\n",
    "            self.models = dict(models)\n",
    "            self.weights = weights or {name: 1.0 for name, _ in models}\n",
    "            \n",
    "        def fit(self, X, y):\n",
    "            # Models are already fitted\n",
    "            return self\n",
    "            \n",
    "        def predict_proba(self, X):\n",
    "            predictions = {}\n",
    "            for name, model in self.models.items():\n",
    "                predictions[name] = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Business-optimized weighting\n",
    "            # Higher weight for models that better separate high-risk cases\n",
    "            ensemble_pred = np.zeros(len(X))\n",
    "            total_weight = sum(self.weights.values())\n",
    "            \n",
    "            for name, weight in self.weights.items():\n",
    "                ensemble_pred += (weight / total_weight) * predictions[name]\n",
    "            \n",
    "            return np.column_stack([1 - ensemble_pred, ensemble_pred])\n",
    "    \n",
    "    # Create business-optimized ensemble\n",
    "    business_weights = {}\n",
    "    for name, model_info in best_models.items():\n",
    "        # Weight based on CV performance and business relevance\n",
    "        business_weights[name] = model_info['cv_score'] * 1.2 if 'XGBoost' in name else model_info['cv_score']\n",
    "    \n",
    "    business_ensemble = BusinessOptimizedEnsemble(models_list, business_weights)\n",
    "    business_ensemble.fit(X_train, y_train)\n",
    "    ensemble_strategies['business_optimized'] = business_ensemble\n",
    "    \n",
    "    print(f\"Created {len(ensemble_strategies)} ensemble strategies\")\n",
    "    return ensemble_strategies\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 5: THRESHOLD OPTIMIZATION WITH BUSINESS CONSTRAINTS\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_thresholds_with_constraints(models, X_val, y_val, scaler):\n",
    "    \"\"\"Advanced threshold optimization with business constraints\"\"\"\n",
    "    \n",
    "    print(\"\\n5. ADVANCED THRESHOLD OPTIMIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def calculate_business_metrics(risk_scores, y_true, low_thresh, high_thresh):\n",
    "        \"\"\"Calculate business metrics for given thresholds\"\"\"\n",
    "        \n",
    "        low_risk = risk_scores < low_thresh\n",
    "        medium_risk = (risk_scores >= low_thresh) & (risk_scores < high_thresh)\n",
    "        high_risk = risk_scores >= high_thresh\n",
    "        \n",
    "        # Business constraints\n",
    "        decline_rate = (medium_risk.sum() + high_risk.sum()) / len(y_true)\n",
    "        agent_alert_rate = high_risk.sum() / len(y_true)\n",
    "        \n",
    "        # Missed fraud rate\n",
    "        fraud_indices = y_true == 1\n",
    "        missed_frauds = (fraud_indices & low_risk).sum()\n",
    "        total_frauds = fraud_indices.sum()\n",
    "        missed_fraud_rate = missed_frauds / total_frauds if total_frauds > 0 else 0\n",
    "        \n",
    "        # Business score (lower is better)\n",
    "        constraint_violations = 0\n",
    "        constraint_violations += max(0, decline_rate - 0.30) * 1000  # Penalty for exceeding 30%\n",
    "        constraint_violations += max(0, agent_alert_rate - 0.001) * 10000  # Penalty for exceeding 0.1%\n",
    "        constraint_violations += max(0, missed_fraud_rate - 0.02) * 5000  # Penalty for exceeding 2%\n",
    "        \n",
    "        return {\n",
    "            'decline_rate': decline_rate,\n",
    "            'agent_alert_rate': agent_alert_rate,\n",
    "            'missed_fraud_rate': missed_fraud_rate,\n",
    "            'constraint_violations': constraint_violations,\n",
    "            'business_score': constraint_violations + missed_fraud_rate * 1000  # Minimize fraud miss\n",
    "        }\n",
    "    \n",
    "    # Grid search for optimal thresholds\n",
    "    print(\"Performing grid search for optimal thresholds...\")\n",
    "    \n",
    "    best_threshold_config = None\n",
    "    best_business_score = float('inf')\n",
    "    \n",
    "    threshold_results = []\n",
    "    \n",
    "    # Calculate risk scores for validation set\n",
    "    risk_scores, _ = calculate_risk_scores(X_val, models, scaler)\n",
    "    \n",
    "    # Grid search\n",
    "    low_thresholds = np.arange(0.05, 0.50, 0.02)\n",
    "    high_thresholds = np.arange(0.20, 0.80, 0.02)\n",
    "    \n",
    "    for low_thresh in low_thresholds:\n",
    "        for high_thresh in high_thresholds:\n",
    "            if high_thresh <= low_thresh:\n",
    "                continue\n",
    "                \n",
    "            metrics = calculate_business_metrics(risk_scores, y_val, low_thresh, high_thresh)\n",
    "            \n",
    "            result = {\n",
    "                'low_threshold': low_thresh,\n",
    "                'high_threshold': high_thresh,\n",
    "                **metrics\n",
    "            }\n",
    "            \n",
    "            threshold_results.append(result)\n",
    "            \n",
    "            # Check if this is the best configuration\n",
    "            if metrics['constraint_violations'] == 0:  # All constraints met\n",
    "                if metrics['business_score'] < best_business_score:\n",
    "                    best_business_score = metrics['business_score']\n",
    "                    best_threshold_config = result\n",
    "    \n",
    "    # If no configuration meets all constraints, find the best compromise\n",
    "    if best_threshold_config is None:\n",
    "        print(\"No threshold combination meets all constraints. Finding best compromise...\")\n",
    "        threshold_df = pd.DataFrame(threshold_results)\n",
    "        \n",
    "        # Find configuration with minimum constraint violations\n",
    "        min_violations = threshold_df['constraint_violations'].min()\n",
    "        best_compromises = threshold_df[threshold_df['constraint_violations'] == min_violations]\n",
    "        best_threshold_config = best_compromises.loc[best_compromises['missed_fraud_rate'].idxmin()].to_dict()\n",
    "    \n",
    "    return best_threshold_config, threshold_results\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 6: FEATURE SELECTION OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_feature_selection(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Advanced feature selection techniques\"\"\"\n",
    "    \n",
    "    print(\"\\n6. FEATURE SELECTION OPTIMIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    feature_selection_methods = {}\n",
    "    \n",
    "    # 1. Correlation-based selection\n",
    "    print(\"Testing correlation-based feature selection...\")\n",
    "    correlation_threshold = 0.01\n",
    "    correlations = pd.DataFrame(X_train).corrwith(pd.Series(y_train)).abs()\n",
    "    selected_features_corr = correlations[correlations > correlation_threshold].index.tolist()\n",
    "    feature_selection_methods['correlation'] = selected_features_corr\n",
    "    \n",
    "    # 2. Mutual Information\n",
    "    print(\"Testing mutual information feature selection...\")\n",
    "    mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "    k_best_mi = SelectKBest(mutual_info_classif, k=min(50, len(mi_scores)))\n",
    "    k_best_mi.fit(X_train, y_train)\n",
    "    selected_features_mi = k_best_mi.get_support(indices=True).tolist()\n",
    "    feature_selection_methods['mutual_info'] = selected_features_mi\n",
    "    \n",
    "    # 3. Recursive Feature Elimination\n",
    "    print(\"Testing recursive feature elimination...\")\n",
    "    rf_selector = RandomForestClassifier(n_estimators=50, random_state=42, class_weight='balanced')\n",
    "    rfe = RFE(rf_selector, n_features_to_select=min(30, X_train.shape[1]), step=5)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features_rfe = rfe.get_support(indices=True).tolist()\n",
    "    feature_selection_methods['rfe'] = selected_features_rfe\n",
    "    \n",
    "    # Evaluate each feature selection method\n",
    "    print(\"\\nEvaluating feature selection methods...\")\n",
    "    feature_selection_results = {}\n",
    "    \n",
    "    for method_name, selected_features in feature_selection_methods.items():\n",
    "        if len(selected_features) > 0:\n",
    "            X_train_selected = X_train[:, selected_features] if hasattr(X_train, 'shape') else X_train.iloc[:, selected_features]\n",
    "            X_val_selected = X_val[:, selected_features] if hasattr(X_val, 'shape') else X_val.iloc[:, selected_features]\n",
    "            \n",
    "            # Quick evaluation\n",
    "            rf_temp = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "            rf_temp.fit(X_train_selected, y_train)\n",
    "            \n",
    "            train_score = rf_temp.score(X_train_selected, y_train)\n",
    "            val_score = rf_temp.score(X_val_selected, y_val)\n",
    "            \n",
    "            feature_selection_results[method_name] = {\n",
    "                'n_features': len(selected_features),\n",
    "                'features': selected_features,\n",
    "                'train_score': train_score,\n",
    "                'val_score': val_score,\n",
    "                'overfitting': train_score - val_score\n",
    "            }\n",
    "            \n",
    "            print(f\"  {method_name}: {len(selected_features)} features, Val Score: {val_score:.4f}\")\n",
    "    \n",
    "    return feature_selection_results\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 7: COST-SENSITIVE LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "def implement_cost_sensitive_learning(X_train, y_train):\n",
    "    \"\"\"Implement cost-sensitive learning approaches\"\"\"\n",
    "    \n",
    "    print(\"\\n7. COST-SENSITIVE LEARNING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Define business costs\n",
    "    # Cost of missing fraud (false negative): High\n",
    "    # Cost of false positive (declining good transaction): Medium\n",
    "    # Cost of agent review: Low but limited capacity\n",
    "    \n",
    "    fraud_rate = y_train.mean()\n",
    "    normal_rate = 1 - fraud_rate\n",
    "    \n",
    "    # Calculate class weights based on business impact\n",
    "    # Higher penalty for missing fraud\n",
    "    class_weights = {\n",
    "        0: 1.0,  # Normal transactions\n",
    "        1: (normal_rate / fraud_rate) * 10  # Fraud transactions - 10x penalty for missing\n",
    "    }\n",
    "    \n",
    "    print(f\"Calculated class weights: {class_weights}\")\n",
    "    \n",
    "    cost_sensitive_models = {}\n",
    "    \n",
    "    # 1. Weighted Random Forest\n",
    "    rf_weighted = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5\n",
    "    )\n",
    "    rf_weighted.fit(X_train, y_train)\n",
    "    cost_sensitive_models['rf_weighted'] = rf_weighted\n",
    "    \n",
    "    # 2. Weighted XGBoost\n",
    "    scale_pos_weight = class_weights[1]\n",
    "    xgb_weighted = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    xgb_weighted.fit(X_train, y_train)\n",
    "    cost_sensitive_models['xgb_weighted'] = xgb_weighted\n",
    "    \n",
    "    # 3. Custom cost matrix implementation\n",
    "    class CostSensitiveClassifier:\n",
    "        def __init__(self, base_model, cost_matrix):\n",
    "            self.base_model = base_model\n",
    "            self.cost_matrix = cost_matrix  # [[TN_cost, FP_cost], [FN_cost, TP_cost]]\n",
    "            \n",
    "        def fit(self, X, y):\n",
    "            self.base_model.fit(X, y)\n",
    "            return self\n",
    "            \n",
    "        def predict_proba(self, X):\n",
    "            base_probs = self.base_model.predict_proba(X)\n",
    "            \n",
    "            # Adjust probabilities based on cost matrix\n",
    "            # Higher cost for false negatives should increase fraud probability\n",
    "            fn_cost = self.cost_matrix[1][0]  # Cost of missing fraud\n",
    "            fp_cost = self.cost_matrix[0][1]  # Cost of false positive\n",
    "            \n",
    "            adjustment_factor = fn_cost / (fn_cost + fp_cost)\n",
    "            adjusted_fraud_prob = base_probs[:, 1] * (1 + adjustment_factor)\n",
    "            adjusted_fraud_prob = np.clip(adjusted_fraud_prob, 0, 1)\n",
    "            \n",
    "            adjusted_normal_prob = 1 - adjusted_fraud_prob\n",
    "            \n",
    "            return np.column_stack([adjusted_normal_prob, adjusted_fraud_prob])\n",
    "    \n",
    "    # Define cost matrix: [[TN, FP], [FN, TP]]\n",
    "    cost_matrix = [[0, 1], [50, 0]]  # Missing fraud costs 50x more than false positive\n",
    "    \n",
    "    base_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    cost_sensitive_rf = CostSensitiveClassifier(base_rf, cost_matrix)\n",
    "    cost_sensitive_rf.fit(X_train, y_train)\n",
    "    cost_sensitive_models['cost_sensitive'] = cost_sensitive_rf\n",
    "    \n",
    "    return cost_sensitive_models, class_weights\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE OPTIMIZATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def comprehensive_model_optimization(df, target_column='Class'):\n",
    "    \"\"\"Complete optimization pipeline\"\"\"\n",
    "    \n",
    "    print(\"STARTING COMPREHENSIVE MODEL OPTIMIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "    \n",
    "    optimization_results = {}\n",
    "    \n",
    "    # 1. Advanced Feature Engineering\n",
    "    df_enhanced = advanced_feature_engineering(df)\n",
    "    X_enhanced = df_enhanced.drop(columns=[target_column])\n",
    "    \n",
    "    # Update splits with enhanced features\n",
    "    X_train_enh = X_enhanced.iloc[X_train.index]\n",
    "    X_val_enh = X_enhanced.iloc[X_val.index]\n",
    "    X_test_enh = X_enhanced.iloc[X_test.index]\n",
    "    \n",
    "    # 2. Scaling optimization\n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'robust': RobustScaler(), \n",
    "        'minmax': MinMaxScaler()\n",
    "    }\n",
    "    \n",
    "    best_scaler = None\n",
    "    best_scaler_score = 0\n",
    "    \n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        X_train_scaled = scaler.fit_transform(X_train_enh)\n",
    "        X_val_scaled = scaler.transform(X_val_enh)\n",
    "        \n",
    "        # Quick evaluation\n",
    "        rf_temp = RandomForestClassifier(n_estimators=50, random_state=42, class_weight='balanced')\n",
    "        rf_temp.fit(X_train_scaled, y_train)\n",
    "        score = rf_temp.score(X_val_scaled, y_val)\n",
    "        \n",
    "        if score > best_scaler_score:\n",
    "            best_scaler_score = score\n",
    "            best_scaler = scaler\n",
    "    \n",
    "    X_train_final = best_scaler.fit_transform(X_train_enh)\n",
    "    X_val_final = best_scaler.transform(X_val_enh)\n",
    "    X_test_final = best_scaler.transform(X_test_enh)\n",
    "    \n",
    "    # 3. Sampling optimization\n",
    "    best_sampler, sampling_results = optimize_sampling_strategy(X_train_final, y_train)\n",
    "    X_train_sampled, y_train_sampled = best_sampler.fit_resample(X_train_final, y_train)\n",
    "    \n",
    "    # 4. Hyperparameter optimization\n",
    "    best_models = optimize_model_hyperparameters(X_train_sampled, y_train_sampled, X_val_final, y_val)\n",
    "    \n",
    "    # 5. Ensemble creation\n",
    "    ensemble_models = create_optimized_ensemble(best_models, X_train_sampled, y_train_sampled)\n",
    "    \n",
    "    # 6. Feature selection\n",
    "    feature_selection_results = optimize_feature_selection(X_train_sampled, y_train_sampled, X_val_final, y_val)\n",
    "    \n",
    "    # 7. Cost-sensitive learning\n",
    "    cost_models, class_weights = implement_cost_sensitive_learning(X_train_sampled, y_train_sampled)\n",
    "    \n",
    "    # Compile results\n",
    "    optimization_results = {\n",
    "        'enhanced_features': df_enhanced.shape[1] - df.shape[1],\n",
    "        'best_scaler': type(best_scaler).__name__,\n",
    "        'sampling_strategy': type(best_sampler).__name__,\n",
    "        'best_models': best_models,\n",
    "        'ensemble_models': list(ensemble_models.keys()),\n",
    "        'feature_selection': feature_selection_results,\n",
    "        'cost_sensitive_models': list(cost_models.keys()),\n",
    "        'class_weights': class_weights,\n",
    "        'final_data_shapes': {\n",
    "            'original': X_train.shape,\n",
    "            'enhanced': X_train_enh.shape,\n",
    "            'sampled': X_train_sampled.shape\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTIMIZATION COMPLETE - SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"âœ“ Feature engineering: Added {optimization_results['enhanced_features']} features\")\n",
    "    print(f\"âœ“ Best scaler: {optimization_results['best_scaler']}\")\n",
    "    print(f\"âœ“ Sampling strategy: {optimization_results['sampling_strategy']}\")\n",
    "    print(f\"âœ“ Optimized models: {len(best_models)}\")\n",
    "    print(f\"âœ“ Ensemble strategies: {len(ensemble_models)}\")\n",
    "    print(f\"âœ“ Feature selection methods: {len(feature_selection_results)}\")\n",
    "    print(f\"âœ“ Cost-sensitive models: {len(cost_models)}\")\n",
    "    \n",
    "    return optimization_results, {\n",
    "        'X_train': X_train_final,\n",
    "        'X_val': X_val_final, \n",
    "        'X_test': X_test_final,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'scaler': best_scaler,\n",
    "        'sampler': best_sampler,\n",
    "        'models': {**best_models, **ensemble_models, **cost_models}\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 8: DYNAMIC THRESHOLD ADJUSTMENT\n",
    "# =============================================================================\n",
    "\n",
    "def implement_dynamic_thresholds(models, X_val, y_val, scaler):\n",
    "    \"\"\"Implement dynamic threshold adjustment based on transaction patterns\"\"\"\n",
    "    \n",
    "    print(\"\\n8. DYNAMIC THRESHOLD ADJUSTMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    class DynamicThresholdClassifier:\n",
    "        def __init__(self, base_models, scaler, base_low_thresh=0.3, base_high_thresh=0.7):\n",
    "            self.base_models = base_models\n",
    "            self.scaler = scaler\n",
    "            self.base_low_thresh = base_low_thresh\n",
    "            self.base_high_thresh = base_high_thresh\n",
    "            self.adjustment_factors = {}\n",
    "            \n",
    "        def fit_adjustments(self, X, y):\n",
    "            \"\"\"Learn adjustment factors based on patterns\"\"\"\n",
    "            \n",
    "            # Calculate base risk scores\n",
    "            risk_scores, individual_scores = calculate_risk_scores(X, self.base_models, self.scaler)\n",
    "            \n",
    "            # Time-based adjustments\n",
    "            if hasattr(X, 'columns') and 'Hour' in X.columns:\n",
    "                hourly_fraud_rates = {}\n",
    "                for hour in range(24):\n",
    "                    hour_mask = X['Hour'] == hour\n",
    "                    if hour_mask.sum() > 0:\n",
    "                        hourly_fraud_rates[hour] = y[hour_mask].mean()\n",
    "                \n",
    "                avg_fraud_rate = y.mean()\n",
    "                self.adjustment_factors['time'] = {}\n",
    "                for hour, rate in hourly_fraud_rates.items():\n",
    "                    # Higher fraud rate hours get lower thresholds (more sensitive)\n",
    "                    adjustment = 1.0 - ((rate / avg_fraud_rate) - 1.0) * 0.2\n",
    "                    self.adjustment_factors['time'][hour] = np.clip(adjustment, 0.7, 1.3)\n",
    "            \n",
    "            # Amount-based adjustments\n",
    "            if hasattr(X, 'columns') and 'Amount' in X.columns:\n",
    "                amount_percentiles = [0, 10, 50, 90, 99, 100]\n",
    "                amount_thresholds = np.percentile(X['Amount'], amount_percentiles)\n",
    "                \n",
    "                self.adjustment_factors['amount'] = {}\n",
    "                for i in range(len(amount_thresholds)-1):\n",
    "                    low_amt, high_amt = amount_thresholds[i], amount_thresholds[i+1]\n",
    "                    amount_mask = (X['Amount'] >= low_amt) & (X['Amount'] < high_amt)\n",
    "                    \n",
    "                    if amount_mask.sum() > 0:\n",
    "                        amt_fraud_rate = y[amount_mask].mean()\n",
    "                        adjustment = 1.0 - ((amt_fraud_rate / avg_fraud_rate) - 1.0) * 0.15\n",
    "                        self.adjustment_factors['amount'][(low_amt, high_amt)] = np.clip(adjustment, 0.8, 1.2)\n",
    "            \n",
    "            return self\n",
    "        \n",
    "        def predict_risk_levels(self, X):\n",
    "            \"\"\"Predict risk levels with dynamic thresholds\"\"\"\n",
    "            \n",
    "            # Get base risk scores\n",
    "            risk_scores, _ = calculate_risk_scores(X, self.base_models, self.scaler)\n",
    "            \n",
    "            # Apply adjustments\n",
    "            adjusted_low_thresh = np.full(len(X), self.base_low_thresh)\n",
    "            adjusted_high_thresh = np.full(len(X), self.base_high_thresh)\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                adjustment = 1.0\n",
    "                \n",
    "                # Time adjustment\n",
    "                if 'time' in self.adjustment_factors and hasattr(X, 'iloc'):\n",
    "                    hour = X.iloc[i]['Hour'] if 'Hour' in X.columns else 12\n",
    "                    adjustment *= self.adjustment_factors['time'].get(hour, 1.0)\n",
    "                \n",
    "                # Amount adjustment\n",
    "                if 'amount' in self.adjustment_factors and hasattr(X, 'iloc'):\n",
    "                    amount = X.iloc[i]['Amount'] if 'Amount' in X.columns else 0\n",
    "                    for (low_amt, high_amt), amt_adj in self.adjustment_factors['amount'].items():\n",
    "                        if low_amt <= amount < high_amt:\n",
    "                            adjustment *= amt_adj\n",
    "                            break\n",
    "                \n",
    "                adjusted_low_thresh[i] *= adjustment\n",
    "                adjusted_high_thresh[i] *= adjustment\n",
    "            \n",
    "            # Classify based on adjusted thresholds\n",
    "            risk_levels = []\n",
    "            actions = []\n",
    "            \n",
    "            for i, score in enumerate(risk_scores):\n",
    "                if score < adjusted_low_thresh[i]:\n",
    "                    risk_levels.append('low')\n",
    "                    actions.append('APPROVE')\n",
    "                elif score < adjusted_high_thresh[i]:\n",
    "                    risk_levels.append('medium')\n",
    "                    actions.append('DECLINE_VALIDATE')\n",
    "                else:\n",
    "                    risk_levels.append('high')\n",
    "                    actions.append('DECLINE_ALERT')\n",
    "            \n",
    "            return risk_levels, actions, {\n",
    "                'risk_scores': risk_scores,\n",
    "                'adjusted_low_thresh': adjusted_low_thresh,\n",
    "                'adjusted_high_thresh': adjusted_high_thresh\n",
    "            }\n",
    "    \n",
    "    # Create and train dynamic classifier\n",
    "    dynamic_classifier = DynamicThresholdClassifier(models, scaler)\n",
    "    dynamic_classifier.fit_adjustments(X_val, y_val)\n",
    "    \n",
    "    return dynamic_classifier\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 9: REAL-TIME MODEL MONITORING\n",
    "# =============================================================================\n",
    "\n",
    "def setup_model_monitoring():\n",
    "    \"\"\"Setup framework for real-time model monitoring\"\"\"\n",
    "    \n",
    "    print(\"\\n9. REAL-TIME MODEL MONITORING SETUP\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    class ModelMonitor:\n",
    "        def __init__(self, target_metrics):\n",
    "            self.target_metrics = target_metrics\n",
    "            self.performance_history = []\n",
    "            self.alert_thresholds = {\n",
    "                'decline_rate': 0.32,  # Alert if > 32% (above 30% target)\n",
    "                'agent_alert_rate': 0.0012,  # Alert if > 0.12% (above 0.1% target)\n",
    "                'missed_fraud_rate': 0.025,  # Alert if > 2.5% (above 2% target)\n",
    "                'model_drift': 0.05  # Alert if performance drops > 5%\n",
    "            }\n",
    "            \n",
    "        def log_performance(self, timestamp, metrics):\n",
    "            \"\"\"Log current performance metrics\"\"\"\n",
    "            self.performance_history.append({\n",
    "                'timestamp': timestamp,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "        def check_alerts(self, current_metrics):\n",
    "            \"\"\"Check if any metrics exceed alert thresholds\"\"\"\n",
    "            alerts = []\n",
    "            \n",
    "            for metric, threshold in self.alert_thresholds.items():\n",
    "                if metric in current_metrics and current_metrics[metric] > threshold:\n",
    "                    alerts.append({\n",
    "                        'metric': metric,\n",
    "                        'current_value': current_metrics[metric],\n",
    "                        'threshold': threshold,\n",
    "                        'severity': 'HIGH' if current_metrics[metric] > threshold * 1.2 else 'MEDIUM'\n",
    "                    })\n",
    "            \n",
    "            return alerts\n",
    "        \n",
    "        def calculate_drift(self, new_predictions, baseline_predictions):\n",
    "            \"\"\"Calculate model drift using PSI (Population Stability Index)\"\"\"\n",
    "            \n",
    "            def calculate_psi(expected, actual, buckets=10):\n",
    "                \"\"\"Calculate Population Stability Index\"\"\"\n",
    "                \n",
    "                # Create buckets\n",
    "                min_val = min(min(expected), min(actual))\n",
    "                max_val = max(max(expected), max(actual))\n",
    "                bucket_bounds = np.linspace(min_val, max_val, buckets + 1)\n",
    "                \n",
    "                # Calculate distributions\n",
    "                expected_dist = np.histogram(expected, bins=bucket_bounds)[0] / len(expected)\n",
    "                actual_dist = np.histogram(actual, bins=bucket_bounds)[0] / len(actual)\n",
    "                \n",
    "                # Add small constant to avoid division by zero\n",
    "                expected_dist = np.where(expected_dist == 0, 0.0001, expected_dist)\n",
    "                actual_dist = np.where(actual_dist == 0, 0.0001, actual_dist)\n",
    "                \n",
    "                # Calculate PSI\n",
    "                psi = np.sum((actual_dist - expected_dist) * np.log(actual_dist / expected_dist))\n",
    "                return psi\n",
    "            \n",
    "            psi = calculate_psi(baseline_predictions, new_predictions)\n",
    "            \n",
    "            drift_level = 'LOW'\n",
    "            if psi > 0.25:\n",
    "                drift_level = 'HIGH'\n",
    "            elif psi > 0.1:\n",
    "                drift_level = 'MEDIUM'\n",
    "            \n",
    "            return {'psi': psi, 'drift_level': drift_level}\n",
    "        \n",
    "        def generate_monitoring_report(self):\n",
    "            \"\"\"Generate comprehensive monitoring report\"\"\"\n",
    "            if not self.performance_history:\n",
    "                return \"No performance history available\"\n",
    "            \n",
    "            recent_performance = self.performance_history[-10:]  # Last 10 records\n",
    "            \n",
    "            report = \"MODEL PERFORMANCE MONITORING REPORT\\n\"\n",
    "            report += \"=\" * 50 + \"\\n\\n\"\n",
    "            \n",
    "            # Current metrics\n",
    "            if recent_performance:\n",
    "                latest = recent_performance[-1]\n",
    "                report += f\"LATEST PERFORMANCE (Timestamp: {latest['timestamp']}):\\n\"\n",
    "                for key, value in latest.items():\n",
    "                    if key != 'timestamp':\n",
    "                        report += f\"  {key}: {value:.4f}\\n\"\n",
    "                \n",
    "                # Check alerts\n",
    "                alerts = self.check_alerts(latest)\n",
    "                if alerts:\n",
    "                    report += \"\\nðŸš¨ ALERTS:\\n\"\n",
    "                    for alert in alerts:\n",
    "                        report += f\"  {alert['severity']}: {alert['metric']} = {alert['current_value']:.4f} (threshold: {alert['threshold']:.4f})\\n\"\n",
    "                else:\n",
    "                    report += \"\\nâœ… All metrics within acceptable ranges\\n\"\n",
    "            \n",
    "            # Trends\n",
    "            if len(recent_performance) > 1:\n",
    "                report += \"\\nTREND ANALYSIS:\\n\"\n",
    "                for metric in ['decline_rate', 'agent_alert_rate', 'missed_fraud_rate']:\n",
    "                    if metric in recent_performance[0]:\n",
    "                        values = [record[metric] for record in recent_performance if metric in record]\n",
    "                        if len(values) > 1:\n",
    "                            trend = \"ðŸ“ˆ INCREASING\" if values[-1] > values[0] else \"ðŸ“‰ DECREASING\"\n",
    "                            change = abs(values[-1] - values[0])\n",
    "                            report += f\"  {metric}: {trend} (Î” {change:.4f})\\n\"\n",
    "            \n",
    "            return report\n",
    "    \n",
    "    # Initialize monitor with target metrics\n",
    "    target_metrics = {\n",
    "        'decline_rate': 0.30,\n",
    "        'agent_alert_rate': 0.001,\n",
    "        'missed_fraud_rate': 0.02\n",
    "    }\n",
    "    \n",
    "    monitor = ModelMonitor(target_metrics)\n",
    "    \n",
    "    print(\"âœ“ Model monitoring framework initialized\")\n",
    "    print(\"âœ“ Alert thresholds configured\")\n",
    "    print(\"âœ“ Drift detection methods ready\")\n",
    "    print(\"âœ“ Reporting system active\")\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 10: A/B TESTING FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "def setup_ab_testing_framework():\n",
    "    \"\"\"Setup A/B testing framework for model deployment\"\"\"\n",
    "    \n",
    "    print(\"\\n10. A/B TESTING FRAMEWORK\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    class ABTestingFramework:\n",
    "        def __init__(self):\n",
    "            self.experiments = {}\n",
    "            self.results = {}\n",
    "            \n",
    "        def create_experiment(self, experiment_name, model_a, model_b, traffic_split=0.5):\n",
    "            \"\"\"Create new A/B test experiment\"\"\"\n",
    "            \n",
    "            self.experiments[experiment_name] = {\n",
    "                'model_a': model_a,\n",
    "                'model_b': model_b,\n",
    "                'traffic_split': traffic_split,\n",
    "                'transactions_a': [],\n",
    "                'transactions_b': [],\n",
    "                'results_a': [],\n",
    "                'results_b': []\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ“ Experiment '{experiment_name}' created with {traffic_split*100}% / {(1-traffic_split)*100}% split\")\n",
    "        \n",
    "        def assign_traffic(self, experiment_name, transaction_id):\n",
    "            \"\"\"Assign transaction to model A or B\"\"\"\n",
    "            \n",
    "            import hashlib\n",
    "            \n",
    "            # Use transaction ID hash for consistent assignment\n",
    "            hash_val = int(hashlib.md5(str(transaction_id).encode()).hexdigest(), 16)\n",
    "            split_point = self.experiments[experiment_name]['traffic_split']\n",
    "            \n",
    "            return 'A' if (hash_val % 100) / 100 < split_point else 'B'\n",
    "        \n",
    "        def log_result(self, experiment_name, transaction_id, model_used, prediction, actual_outcome, business_metrics):\n",
    "            \"\"\"Log experiment result\"\"\"\n",
    "            \n",
    "            experiment = self.experiments[experiment_name]\n",
    "            \n",
    "            result = {\n",
    "                'transaction_id': transaction_id,\n",
    "                'prediction': prediction,\n",
    "                'actual_outcome': actual_outcome,\n",
    "                'business_metrics': business_metrics,\n",
    "                'timestamp': pd.Timestamp.now()\n",
    "            }\n",
    "            \n",
    "            if model_used == 'A':\n",
    "                experiment['transactions_a'].append(transaction_id)\n",
    "                experiment['results_a'].append(result)\n",
    "            else:\n",
    "                experiment['transactions_b'].append(transaction_id)\n",
    "                experiment['results_b'].append(result)\n",
    "        \n",
    "        def calculate_experiment_results(self, experiment_name, min_sample_size=1000):\n",
    "            \"\"\"Calculate statistical significance of experiment results\"\"\"\n",
    "            \n",
    "            experiment = self.experiments[experiment_name]\n",
    "            results_a = experiment['results_a']\n",
    "            results_b = experiment['results_b']\n",
    "            \n",
    "            if len(results_a) < min_sample_size or len(results_b) < min_sample_size:\n",
    "                return {\n",
    "                    'status': 'INSUFFICIENT_DATA',\n",
    "                    'sample_size_a': len(results_a),\n",
    "                    'sample_size_b': len(results_b),\n",
    "                    'min_required': min_sample_size\n",
    "                }\n",
    "            \n",
    "            # Calculate key metrics for both groups\n",
    "            metrics_a = self._calculate_group_metrics(results_a)\n",
    "            metrics_b = self._calculate_group_metrics(results_b)\n",
    "            \n",
    "            # Statistical significance testing\n",
    "            from scipy import stats\n",
    "            \n",
    "            significance_results = {}\n",
    "            \n",
    "            for metric in ['decline_rate', 'agent_alert_rate', 'missed_fraud_rate']:\n",
    "                if metric in metrics_a and metric in metrics_b:\n",
    "                    # Two-proportion z-test\n",
    "                    successes_a = metrics_a[metric] * len(results_a)\n",
    "                    successes_b = metrics_b[metric] * len(results_b)\n",
    "                    \n",
    "                    # Simplified significance test\n",
    "                    p_value = stats.chi2_contingency([\n",
    "                        [successes_a, len(results_a) - successes_a],\n",
    "                        [successes_b, len(results_b) - successes_b]\n",
    "                    ])[1]\n",
    "                    \n",
    "                    significance_results[metric] = {\n",
    "                        'model_a': metrics_a[metric],\n",
    "                        'model_b': metrics_b[metric],\n",
    "                        'difference': metrics_b[metric] - metrics_a[metric],\n",
    "                        'p_value': p_value,\n",
    "                        'significant': p_value < 0.05\n",
    "                    }\n",
    "            \n",
    "            return {\n",
    "                'status': 'COMPLETE',\n",
    "                'sample_size_a': len(results_a),\n",
    "                'sample_size_b': len(results_b),\n",
    "                'metrics_a': metrics_a,\n",
    "                'metrics_b': metrics_b,\n",
    "                'significance_tests': significance_results,\n",
    "                'recommendation': self._generate_recommendation(significance_results)\n",
    "            }\n",
    "        \n",
    "        def _calculate_group_metrics(self, results):\n",
    "            \"\"\"Calculate metrics for a group of results\"\"\"\n",
    "            \n",
    "            if not results:\n",
    "                return {}\n",
    "            \n",
    "            total_transactions = len(results)\n",
    "            declined = sum(1 for r in results if r['business_metrics'].get('action') in ['DECLINE_VALIDATE', 'DECLINE_ALERT'])\n",
    "            agent_alerts = sum(1 for r in results if r['business_metrics'].get('action') == 'DECLINE_ALERT')\n",
    "            \n",
    "            # Calculate fraud metrics\n",
    "            actual_frauds = sum(1 for r in results if r['actual_outcome'] == 1)\n",
    "            missed_frauds = sum(1 for r in results if r['actual_outcome'] == 1 and r['business_metrics'].get('action') == 'APPROVE')\n",
    "            \n",
    "            return {\n",
    "                'decline_rate': declined / total_transactions if total_transactions > 0 else 0,\n",
    "                'agent_alert_rate': agent_alerts / total_transactions if total_transactions > 0 else 0,\n",
    "                'missed_fraud_rate': missed_frauds / actual_frauds if actual_frauds > 0 else 0,\n",
    "                'total_transactions': total_transactions,\n",
    "                'total_frauds': actual_frauds\n",
    "            }\n",
    "        \n",
    "        def _generate_recommendation(self, significance_results):\n",
    "            \"\"\"Generate recommendation based on test results\"\"\"\n",
    "            \n",
    "            if not significance_results:\n",
    "                return \"Insufficient data for recommendation\"\n",
    "            \n",
    "            # Check if Model B is significantly better\n",
    "            better_metrics = 0\n",
    "            worse_metrics = 0\n",
    "            \n",
    "            for metric, result in significance_results.items():\n",
    "                if result['significant']:\n",
    "                    if metric == 'missed_fraud_rate':\n",
    "                        # Lower is better for missed fraud\n",
    "                        if result['difference'] < 0:\n",
    "                            better_metrics += 1\n",
    "                        else:\n",
    "                            worse_metrics += 1\n",
    "                    else:\n",
    "                        # Lower is generally better for decline and alert rates\n",
    "                        # But need to balance with business constraints\n",
    "                        if abs(result['difference']) > 0.01:  # Meaningful difference\n",
    "                            if result['difference'] < 0:\n",
    "                                better_metrics += 1\n",
    "                            else:\n",
    "                                worse_metrics += 1\n",
    "            \n",
    "            if better_metrics > worse_metrics:\n",
    "                return \"DEPLOY MODEL B - Significantly better performance\"\n",
    "            elif worse_metrics > better_metrics:\n",
    "                return \"KEEP MODEL A - Model B performs worse\"\n",
    "            else:\n",
    "                return \"NO CLEAR WINNER - Consider longer test or different models\"\n",
    "    \n",
    "    # Initialize A/B testing framework\n",
    "    ab_framework = ABTestingFramework()\n",
    "    \n",
    "    print(\"âœ“ A/B testing framework initialized\")\n",
    "    print(\"âœ“ Statistical significance testing ready\")\n",
    "    print(\"âœ“ Business metrics tracking configured\")\n",
    "    print(\"âœ“ Recommendation engine active\")\n",
    "    \n",
    "    return ab_framework\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL INTEGRATION AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_optimization_recommendations():\n",
    "    \"\"\"Generate comprehensive optimization recommendations\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE OPTIMIZATION RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    recommendations = {\n",
    "        'immediate_actions': [\n",
    "            \"1. Implement advanced feature engineering (time-based, amount-based, PCA combinations)\",\n",
    "            \"2. Apply SMOTE or BorderlineSMOTE sampling to handle class imbalance\",\n",
    "            \"3. Use ensemble methods (XGBoost + RandomForest + LightGBM)\",\n",
    "            \"4. Implement cost-sensitive learning with 50:1 fraud penalty ratio\",\n",
    "            \"5. Optimize thresholds using grid search with business constraints\"\n",
    "        ],\n",
    "        \n",
    "        'model_improvements': [\n",
    "            \"1. Use gradient boosting models (XGBoost/LightGBM) for better performance\",\n",
    "            \"2. Implement dynamic threshold adjustment based on time/amount patterns\",\n",
    "            \"3. Create weighted ensemble with performance-based voting\",\n",
    "            \"4. Apply recursive feature elimination to reduce overfitting\",\n",
    "            \"5. Use RobustScaler for better handling of outliers\"\n",
    "        ],\n",
    "        \n",
    "        'business_alignment': [\n",
    "            \"1. Implement three-tier risk classification system\",\n",
    "            \"2. Set up real-time monitoring for constraint violations\",\n",
    "            \"3. Create A/B testing framework for model deployment\",\n",
    "            \"4. Establish feedback loops for continuous learning\",\n",
    "            \"5. Implement customer validation workflows for medium-risk transactions\"\n",
    "        ],\n",
    "        \n",
    "        'technical_infrastructure': [\n",
    "            \"1. Deploy real-time scoring API with < 100ms latency\",\n",
    "            \"2. Implement model versioning and rollback capabilities\",\n",
    "            \"3. Set up automated retraining pipelines (weekly/monthly)\",\n",
    "            \"4. Create monitoring dashboards for business stakeholders\",\n",
    "            \"5. Implement data drift detection and alerting\"\n",
    "        ],\n",
    "        \n",
    "        'risk_management': [\n",
    "            \"1. Establish model governance and approval processes\",\n",
    "            \"2. Implement challenger model framework\",\n",
    "            \"3. Create escalation procedures for high-risk transactions\",\n",
    "            \"4. Set up regular model validation and backtesting\",\n",
    "            \"5. Implement fairness and bias monitoring\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ IMMEDIATE ACTIONS (Next 2 weeks):\")\n",
    "    for action in recommendations['immediate_actions']:\n",
    "        print(f\"   {action}\")\n",
    "    \n",
    "    print(\"\\nðŸ”§ MODEL IMPROVEMENTS (Next month):\")\n",
    "    for improvement in recommendations['model_improvements']:\n",
    "        print(f\"   {improvement}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¼ BUSINESS ALIGNMENT (Next quarter):\")\n",
    "    for alignment in recommendations['business_alignment']:\n",
    "        print(f\"   {alignment}\")\n",
    "    \n",
    "    print(\"\\nðŸ—ï¸ TECHNICAL INFRASTRUCTURE (Next 3 months):\")\n",
    "    for infra in recommendations['technical_infrastructure']:\n",
    "        print(f\"   {infra}\")\n",
    "    \n",
    "    print(\"\\nâš–ï¸ RISK MANAGEMENT (Ongoing):\")\n",
    "    for risk in recommendations['risk_management']:\n",
    "        print(f\"   {risk}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUCCESS METRICS TO TRACK:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    success_metrics = [\n",
    "        \"âœ… Decline Rate: Currently vs Target â‰¤ 30%\",\n",
    "        \"âœ… Agent Alert Rate: Currently vs Target < 0.1%\", \n",
    "        \"âœ… Missed Fraud Rate: Currently vs Target â‰¤ 2%\",\n",
    "        \"ðŸ“ˆ Model Performance: AUC-ROC improvement\",\n",
    "        \"ðŸ’° Business Impact: Cost reduction from automation\",\n",
    "        \"â±ï¸ Processing Speed: Average decision time\",\n",
    "        \"ðŸŽ¯ Customer Satisfaction: Reduced false positives\",\n",
    "        \"ðŸ” Model Stability: Prediction consistency over time\"\n",
    "    ]\n",
    "    \n",
    "    for metric in success_metrics:\n",
    "        print(f\"   {metric}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Remember: Optimization is an iterative process!\")\n",
    "    print(\"Start with immediate actions and gradually implement advanced features.\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"To use these optimization strategies:\")\n",
    "    print(\"1. Run comprehensive_model_optimization(your_dataframe)\")\n",
    "    print(\"2. Implement the recommendations based on priority\")\n",
    "    print(\"3. Set up monitoring and A/B testing frameworks\")\n",
    "    print(\"4. Continuously iterate based on performance feedback\")\n",
    "    \n",
    "    # Generate final recommendations\n",
    "    recommendations = generate_optimization_recommendations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
