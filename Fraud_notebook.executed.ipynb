{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f647903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T21:18:10.041751Z",
     "iopub.status.busy": "2025-09-12T21:18:10.041500Z",
     "iopub.status.idle": "2025-09-12T21:18:10.044788Z",
     "shell.execute_reply": "2025-09-12T21:18:10.044189Z"
    },
    "papermill": {
     "duration": 0.007483,
     "end_time": "2025-09-12T21:18:10.046176",
     "exception": false,
     "start_time": "2025-09-12T21:18:10.038693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# jupytext:\n",
    "#   formats: py:percent,ipynb\n",
    "#   text_representation:\n",
    "#     extension: .py\n",
    "#     format_name: percent\n",
    "#     format_version: '1.3'\n",
    "#     jupytext_version: '1.16.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85acade",
   "metadata": {
    "papermill": {
     "duration": 0.001778,
     "end_time": "2025-09-12T21:18:10.050927",
     "exception": false,
     "start_time": "2025-09-12T21:18:10.049149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credit Card Fraud Detection with Risk-Based Rules\n",
    "\n",
    "This script is structured with Jupytext cell markers so it can be converted to a Jupyter Notebook\n",
    "and executed to persist rich outputs in GitHub. To convert and execute:\n",
    "1) jupytext --to ipynb Fraud_notebook.py\n",
    "2) papermill Fraud_notebook.ipynb Fraud_notebook.executed.ipynb\n",
    "\n",
    "Credit Card Fraud Detection with Risk-Based Rules\n",
    "Objective: Define rules for credit card fraud with three risk levels:\n",
    "- Low risk: Accept\n",
    "- Medium risk: Decline and ask customer to validate  \n",
    "- High risk: Decline and alert agent\n",
    "\n",
    "Constraints:\n",
    "- Decline rate <= 30% of all transactions\n",
    "- Agent alerts < 0.1% of all transactions  \n",
    "- Missed fraud <= 0.02%\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Credit Card Fraud Detection System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "Load the dataset\n",
    "Note: Download creditcard.csv from https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "try:\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Fraud rate: {df['Class'].mean():.4f} ({df['Class'].sum()} frauds out of {len(df)} transactions)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please download 'creditcard.csv' from Kaggle and place it in the current directory\")\n",
    "    print(\"Dataset URL: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\")\n",
    "    # Create synthetic data for demonstration\n",
    "    print(\"\\nCreating synthetic dataset for demonstration...\")\n",
    "    from sklearn.datasets import make_classification\n",
    "    X_synthetic, y_synthetic = make_classification(\n",
    "        n_samples=50000, n_features=30, n_informative=15, n_redundant=10,\n",
    "        n_classes=2, weights=[0.998, 0.002], flip_y=0.01, random_state=42\n",
    "    )\n",
    "    df = pd.DataFrame(X_synthetic, columns=[f'V{i}' for i in range(1, 29)] + ['Time', 'Amount'])\n",
    "    df['Class'] = y_synthetic\n",
    "    df['Time'] = np.random.uniform(0, 172800, len(df))  # 2 days in seconds\n",
    "    df['Amount'] = np.random.lognormal(3, 1.5, len(df))\n",
    "    print(f\"Synthetic dataset created: {df.shape}\")\n",
    "\n",
    "=============================================================================\n",
    "EXPLORATORY DATA ANALYSIS\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "Basic statistics\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Total transactioxns: {len(df):,}\")\n",
    "print(f\"Fraudulent transactions: {df['Class'].sum():,}\")\n",
    "print(f\"Fraud rate: {df['Class'].mean():.4f} ({df['Class'].mean()*100:.2f}%)\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "Class distribution\n",
    "fraud_counts = df['Class'].value_counts()\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"Normal transactions: {fraud_counts[0]:,} ({fraud_counts[0]/len(df)*100:.2f}%)\")\n",
    "print(f\"Fraud transactions: {fraud_counts[1]:,} ({fraud_counts[1]/len(df)*100:.2f}%)\")\n",
    "\n",
    "Amount analysis\n",
    "print(f\"\\nTransaction Amount Analysis:\")\n",
    "print(f\"Normal transactions - Mean: ${df[df['Class']==0]['Amount'].mean():.2f}, Median: ${df[df['Class']==0]['Amount'].median():.2f}\")\n",
    "print(f\"Fraud transactions - Mean: ${df[df['Class']==1]['Amount'].mean():.2f}, Median: ${df[df['Class']==1]['Amount'].median():.2f}\")\n",
    "\n",
    "Time analysis (convert to hours)\n",
    "df['Hour'] = (df['Time'] % (24*3600)) // 3600\n",
    "\n",
    "=============================================================================\n",
    "FEATURE ENGINEERING\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "Create additional features for rule-based detection\n",
    "def create_features(data):\n",
    "    \"\"\"Create additional features for fraud detection\"\"\"\n",
    "    df_features = data.copy()\n",
    "    \n",
    "    # Amount-based features\n",
    "    df_features['Amount_log'] = np.log1p(df_features['Amount'])\n",
    "    df_features['Amount_sqrt'] = np.sqrt(df_features['Amount'])\n",
    "    \n",
    "    # Amount categories\n",
    "    df_features['Amount_category'] = pd.cut(df_features['Amount'], \n",
    "                                           bins=[0, 10, 100, 1000, float('inf')], \n",
    "                                           labels=['Very_Low', 'Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Time-based features\n",
    "    df_features['Hour_sin'] = np.sin(2 * np.pi * df_features['Hour'] / 24)\n",
    "    df_features['Hour_cos'] = np.cos(2 * np.pi * df_features['Hour'] / 24)\n",
    "    df_features['Is_Night'] = ((df_features['Hour'] >= 23) | (df_features['Hour'] <= 6)).astype(int)\n",
    "    df_features['Is_Weekend'] = ((df_features['Time'] // (24*3600)) % 7 >= 5).astype(int)\n",
    "    \n",
    "    # PCA feature combinations (for V1-V28)\n",
    "    v_cols = [col for col in df_features.columns if col.startswith('V')]\n",
    "    if len(v_cols) >= 28:\n",
    "        df_features['V_sum_pos'] = df_features[v_cols].clip(lower=0).sum(axis=1)\n",
    "        df_features['V_sum_neg'] = df_features[v_cols].clip(upper=0).sum(axis=1)\n",
    "        df_features['V_mean'] = df_features[v_cols].mean(axis=1)\n",
    "        df_features['V_std'] = df_features[v_cols].std(axis=1)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "Apply feature engineering\n",
    "df_features = create_features(df)\n",
    "print(f\"Features created. New shape: {df_features.shape}\")\n",
    "\n",
    "=============================================================================\n",
    "MODEL TRAINING\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "Prepare features for modeling\n",
    "feature_cols = [col for col in df_features.columns if col not in ['Class', 'Amount_category']]\n",
    "X = df_features[feature_cols]\n",
    "y = df_features['Class']\n",
    "\n",
    "Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    stratify=y, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} transactions\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} transactions\")\n",
    "print(f\"Training fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"Test fraud rate: {y_test.mean():.4f}\")\n",
    "\n",
    "Scale features\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "Train multiple models for ensemble approach\n",
    "models = {}\n",
    "\n",
    "print(\"\\nTraining models...\")\n",
    "\n",
    "1. Logistic Regression\n",
    "print(\"- Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "models['logistic'] = lr_model\n",
    "\n",
    "2. Random Forest  \n",
    "print(\"- Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', \n",
    "                                 random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "models['random_forest'] = rf_model\n",
    "\n",
    "3. Isolation Forest (for anomaly detection)\n",
    "print(\"- Training Isolation Forest...\")\n",
    "iso_model = IsolationForest(contamination=0.002, random_state=42, n_jobs=-1)\n",
    "iso_model.fit(X_train_scaled[y_train == 0])  # Train only on normal transactions\n",
    "models['isolation'] = iso_model\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "=============================================================================\n",
    "RISK SCORE CALCULATION\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RISK SCORE CALCULATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def calculate_risk_scores(X_data, models, scaler):\n",
    "    \"\"\"Calculate comprehensive risk scores using multiple models\"\"\"\n",
    "    X_scaled = scaler.transform(X_data)\n",
    "    \n",
    "    # Get probabilities from different models\n",
    "    lr_probs = models['logistic'].predict_proba(X_scaled)[:, 1]\n",
    "    rf_probs = models['random_forest'].predict_proba(X_scaled)[:, 1]\n",
    "    iso_scores = models['isolation'].decision_function(X_scaled)\n",
    "    \n",
    "    # Normalize isolation forest scores to 0-1 range\n",
    "    iso_scores_norm = (iso_scores - iso_scores.min()) / (iso_scores.max() - iso_scores.min())\n",
    "    iso_scores_norm = 1 - iso_scores_norm  # Invert so higher = more anomalous\n",
    "    \n",
    "    # Ensemble risk score (weighted combination)\n",
    "    risk_scores = (0.4 * lr_probs + 0.4 * rf_probs + 0.2 * iso_scores_norm)\n",
    "    \n",
    "    return risk_scores, {\n",
    "        'logistic_prob': lr_probs,\n",
    "        'rf_prob': rf_probs, \n",
    "        'isolation_score': iso_scores_norm\n",
    "    }\n",
    "\n",
    "Calculate risk scores for test set\n",
    "test_risk_scores, test_individual_scores = calculate_risk_scores(X_test, models, scaler)\n",
    "\n",
    "print(f\"Risk scores calculated for {len(test_risk_scores):,} transactions\")\n",
    "print(f\"Risk score range: {test_risk_scores.min():.4f} - {test_risk_scores.max():.4f}\")\n",
    "print(f\"Mean risk score: {test_risk_scores.mean():.4f}\")\n",
    "\n",
    "=============================================================================\n",
    "RISK THRESHOLD OPTIMIZATION\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RISK THRESHOLD OPTIMIZATION\")  \n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_thresholds(risk_scores, y_true, low_threshold, high_threshold):\n",
    "    \"\"\"Evaluate risk-based rules with given thresholds\"\"\"\n",
    "    # Classify transactions\n",
    "    low_risk = risk_scores < low_threshold\n",
    "    medium_risk = (risk_scores >= low_threshold) & (risk_scores < high_threshold)\n",
    "    high_risk = risk_scores >= high_threshold\n",
    "    \n",
    "    total_transactions = len(y_true)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'total_transactions': total_transactions,\n",
    "        'low_risk_count': low_risk.sum(),\n",
    "        'medium_risk_count': medium_risk.sum(),\n",
    "        'high_risk_count': high_risk.sum(),\n",
    "        'low_risk_pct': low_risk.mean() * 100,\n",
    "        'medium_risk_pct': medium_risk.mean() * 100,\n",
    "        'high_risk_pct': high_risk.mean() * 100\n",
    "    }\n",
    "    \n",
    "    # Decline rate (medium + high risk)\n",
    "    decline_rate = (medium_risk.sum() + high_risk.sum()) / total_transactions\n",
    "    \n",
    "    # Agent alert rate (high risk only)\n",
    "    agent_alert_rate = high_risk.sum() / total_transactions\n",
    "    \n",
    "    # Missed fraud rate (frauds classified as low risk)\n",
    "    fraud_indices = y_true == 1\n",
    "    missed_frauds = (fraud_indices & low_risk).sum()\n",
    "    total_frauds = fraud_indices.sum()\n",
    "    missed_fraud_rate = missed_frauds / total_frauds if total_frauds > 0 else 0\n",
    "    \n",
    "    # Fraud detection by risk category\n",
    "    low_risk_frauds = (fraud_indices & low_risk).sum()\n",
    "    medium_risk_frauds = (fraud_indices & medium_risk).sum()\n",
    "    high_risk_frauds = (fraud_indices & high_risk).sum()\n",
    "    \n",
    "    results.update({\n",
    "        'decline_rate': decline_rate,\n",
    "        'agent_alert_rate': agent_alert_rate,\n",
    "        'missed_fraud_rate': missed_fraud_rate,\n",
    "        'low_risk_frauds': low_risk_frauds,\n",
    "        'medium_risk_frauds': medium_risk_frauds,\n",
    "        'high_risk_frauds': high_risk_frauds,\n",
    "        'total_frauds': total_frauds\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "Test different threshold combinations\n",
    "print(\"Optimizing thresholds to meet constraints...\")\n",
    "print(\"Constraints: Decline rate ≤ 30%, Agent alerts < 0.1%, Missed fraud ≤ 0.02%\")\n",
    "\n",
    "best_thresholds = None\n",
    "best_results = None\n",
    "threshold_tests = []\n",
    "\n",
    "Test various threshold combinations\n",
    "for low_thresh in np.arange(0.1, 0.8, 0.05):\n",
    "    for high_thresh in np.arange(low_thresh + 0.05, 0.95, 0.05):\n",
    "        results = evaluate_thresholds(test_risk_scores, y_test, low_thresh, high_thresh)\n",
    "        \n",
    "        # Check if constraints are met\n",
    "        meets_constraints = (\n",
    "            results['decline_rate'] <= 0.30 and  # Decline rate ≤ 30%\n",
    "            results['agent_alert_rate'] < 0.001 and  # Agent alerts < 0.1%\n",
    "            results['missed_fraud_rate'] <= 0.02  # Missed fraud ≤ 2%\n",
    "        )\n",
    "        \n",
    "        results['low_threshold'] = low_thresh\n",
    "        results['high_threshold'] = high_thresh\n",
    "        results['meets_constraints'] = meets_constraints\n",
    "        \n",
    "        threshold_tests.append(results)\n",
    "        \n",
    "        if meets_constraints and (best_results is None or \n",
    "                                results['missed_fraud_rate'] < best_results['missed_fraud_rate']):\n",
    "            best_thresholds = (low_thresh, high_thresh)\n",
    "            best_results = results\n",
    "\n",
    "Display results\n",
    "if best_thresholds:\n",
    "    low_thresh, high_thresh = best_thresholds\n",
    "    print(f\"\\nOPTIMAL THRESHOLDS FOUND:\")\n",
    "    print(f\"Low risk threshold: {low_thresh:.3f}\")\n",
    "    print(f\"High risk threshold: {high_thresh:.3f}\")\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE METRICS:\")\n",
    "    print(f\"Decline rate: {best_results['decline_rate']*100:.2f}%\")\n",
    "    print(f\"Agent alert rate: {best_results['agent_alert_rate']*100:.4f}%\") \n",
    "    print(f\"Missed fraud rate: {best_results['missed_fraud_rate']*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nRISK DISTRIBUTION:\")\n",
    "    print(f\"Low risk: {best_results['low_risk_count']:,} transactions ({best_results['low_risk_pct']:.1f}%)\")\n",
    "    print(f\"Medium risk: {best_results['medium_risk_count']:,} transactions ({best_results['medium_risk_pct']:.1f}%)\")\n",
    "    print(f\"High risk: {best_results['high_risk_count']:,} transactions ({best_results['high_risk_pct']:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nFRAUD DETECTION BY RISK LEVEL:\")\n",
    "    print(f\"Low risk frauds: {best_results['low_risk_frauds']} (missed)\")\n",
    "    print(f\"Medium risk frauds: {best_results['medium_risk_frauds']} (customer validation)\")\n",
    "    print(f\"High risk frauds: {best_results['high_risk_frauds']} (agent review)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo threshold combination found that meets all constraints!\")\n",
    "    print(\"Consider relaxing constraints or improving the model.\")\n",
    "    \n",
    "    # Show best alternatives\n",
    "    df_results = pd.DataFrame(threshold_tests)\n",
    "    df_results = df_results.sort_values('missed_fraud_rate')\n",
    "    \n",
    "    print(\"\\nBest alternatives (lowest missed fraud rate):\")\n",
    "    print(df_results[['low_threshold', 'high_threshold', 'decline_rate', \n",
    "                     'agent_alert_rate', 'missed_fraud_rate']].head().round(4))\n",
    "\n",
    "=============================================================================\n",
    "BUSINESS RULES DEFINITION  \n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUSINESS RULES DEFINITION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if best_thresholds:\n",
    "    low_thresh, high_thresh = best_thresholds\n",
    "    \n",
    "    print(\"FRAUD DETECTION BUSINESS RULES\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Rule 1 - LOW RISK (Accept Transaction):\")\n",
    "    print(f\"  • Risk Score < {low_thresh:.3f}\")\n",
    "    print(f\"  • Action: APPROVE transaction automatically\")\n",
    "    print(f\"  • Expected volume: ~{best_results['low_risk_pct']:.1f}% of transactions\")\n",
    "    \n",
    "    print(f\"\\nRule 2 - MEDIUM RISK (Customer Validation):\")\n",
    "    print(f\"  • Risk Score >= {low_thresh:.3f} AND < {high_thresh:.3f}\")\n",
    "    print(f\"  • Action: DECLINE transaction, request customer validation\")\n",
    "    print(f\"  • Expected volume: ~{best_results['medium_risk_pct']:.1f}% of transactions\")\n",
    "    print(f\"  • Customer actions: SMS/Email verification, phone call, etc.\")\n",
    "    \n",
    "    print(f\"\\nRule 3 - HIGH RISK (Agent Review):\")\n",
    "    print(f\"  • Risk Score >= {high_thresh:.3f}\")\n",
    "    print(f\"  • Action: DECLINE transaction, create agent alert\")\n",
    "    print(f\"  • Expected volume: ~{best_results['high_risk_pct']:.1f}% of transactions\")\n",
    "    print(f\"  • Agent actions: Manual review, contact customer, investigate pattern\")\n",
    "    \n",
    "    print(f\"\\nSYSTEM PERFORMANCE:\")\n",
    "    print(f\"✓ Decline rate: {best_results['decline_rate']*100:.1f}% (≤ 30% ✓)\")\n",
    "    print(f\"✓ Agent alerts: {best_results['agent_alert_rate']*100:.3f}% (< 0.1% ✓)\")\n",
    "    print(f\"✓ Missed fraud: {best_results['missed_fraud_rate']*100:.1f}% (≤ 2% ✓)\")\n",
    "\n",
    "=============================================================================\n",
    "RISK SCORE IMPLEMENTATION FUNCTION\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PRODUCTION IMPLEMENTATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def fraud_risk_classifier(transaction_features, models, scaler, low_threshold, high_threshold):\n",
    "    \"\"\"\n",
    "    Production function to classify transactions by fraud risk\n",
    "    \n",
    "    Parameters:\n",
    "    - transaction_features: DataFrame with transaction features\n",
    "    - models: Dictionary of trained models\n",
    "    - scaler: Fitted feature scaler\n",
    "    - low_threshold: Threshold between low and medium risk\n",
    "    - high_threshold: Threshold between medium and high risk\n",
    "    \n",
    "    Returns:\n",
    "    - risk_level: 'low', 'medium', or 'high'\n",
    "    - risk_score: Numerical risk score (0-1)\n",
    "    - action: Recommended action\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate risk score\n",
    "    risk_scores, _ = calculate_risk_scores(transaction_features, models, scaler)\n",
    "    risk_score = risk_scores[0] if len(risk_scores) == 1 else risk_scores\n",
    "    \n",
    "    # Determine risk level and action\n",
    "    if isinstance(risk_score, np.ndarray):\n",
    "        risk_levels = []\n",
    "        actions = []\n",
    "        for score in risk_score:\n",
    "            if score < low_threshold:\n",
    "                risk_levels.append('low')\n",
    "                actions.append('APPROVE')\n",
    "            elif score < high_threshold:\n",
    "                risk_levels.append('medium')\n",
    "                actions.append('DECLINE_VALIDATE')\n",
    "            else:\n",
    "                risk_levels.append('high')\n",
    "                actions.append('DECLINE_ALERT')\n",
    "        return risk_levels, risk_score, actions\n",
    "    else:\n",
    "        if risk_score < low_threshold:\n",
    "            return 'low', risk_score, 'APPROVE'\n",
    "        elif risk_score < high_threshold:\n",
    "            return 'medium', risk_score, 'DECLINE_VALIDATE'\n",
    "        else:\n",
    "            return 'high', risk_score, 'DECLINE_ALERT'\n",
    "\n",
    "Example usage\n",
    "if best_thresholds:\n",
    "    print(\"PRODUCTION FUNCTION EXAMPLE:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Test with a few sample transactions\n",
    "    sample_transactions = X_test.head(5)\n",
    "    risk_levels, risk_scores, actions = fraud_risk_classifier(\n",
    "        sample_transactions, models, scaler, low_thresh, high_thresh\n",
    "    )\n",
    "    \n",
    "    for i, (level, score, action) in enumerate(zip(risk_levels, risk_scores, actions)):\n",
    "        actual_fraud = 'FRAUD' if y_test.iloc[i] == 1 else 'NORMAL'\n",
    "        print(f\"Transaction {i+1}: Risk={level.upper()} (Score={score:.4f}) → {action} | Actual: {actual_fraud}\")\n",
    "\n",
    "=============================================================================\n",
    "VISUALIZATION\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "if best_thresholds:\n",
    "    # Plot 1: Risk Score Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(test_risk_scores[y_test == 0], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "    ax1.hist(test_risk_scores[y_test == 1], bins=50, alpha=0.7, label='Fraud', density=True)\n",
    "    ax1.axvline(low_thresh, color='orange', linestyle='--', label=f'Low Threshold ({low_thresh:.3f})')\n",
    "    ax1.axvline(high_thresh, color='red', linestyle='--', label=f'High Threshold ({high_thresh:.3f})')\n",
    "    ax1.set_xlabel('Risk Score')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Risk Score Distribution by Class')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Transaction Volume by Risk Level\n",
    "    ax2 = axes[0, 1]\n",
    "    risk_counts = [best_results['low_risk_count'], best_results['medium_risk_count'], best_results['high_risk_count']]\n",
    "    risk_labels = ['Low Risk\\n(Approve)', 'Medium Risk\\n(Validate)', 'High Risk\\n(Alert)']\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    bars = ax2.bar(risk_labels, risk_counts, color=colors, alpha=0.7)\n",
    "    ax2.set_ylabel('Number of Transactions')\n",
    "    ax2.set_title('Transaction Volume by Risk Level')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, count in zip(bars, risk_counts):\n",
    "        percentage = count / sum(risk_counts) * 100\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + sum(risk_counts)*0.01, \n",
    "                f'{percentage:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Fraud Detection by Risk Level\n",
    "    ax3 = axes[1, 0]\n",
    "    fraud_counts = [best_results['low_risk_frauds'], best_results['medium_risk_frauds'], best_results['high_risk_frauds']]\n",
    "    bars = ax3.bar(risk_labels, fraud_counts, color=colors, alpha=0.7)\n",
    "    ax3.set_ylabel('Number of Fraudulent Transactions')\n",
    "    ax3.set_title('Fraud Detection by Risk Level')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, fraud_counts):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(fraud_counts)*0.05, \n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Performance Metrics\n",
    "    ax4 = axes[1, 1]\n",
    "    metrics = ['Decline Rate', 'Agent Alert Rate', 'Missed Fraud Rate']\n",
    "    values = [best_results['decline_rate']*100, best_results['agent_alert_rate']*100, best_results['missed_fraud_rate']*100]\n",
    "    constraints = [30, 0.1, 2]  # Constraint values in percentages\n",
    "    \n",
    "    x_pos = np.arange(len(metrics))\n",
    "    bars = ax4.bar(x_pos, values, color=['orange', 'red', 'purple'], alpha=0.7, label='Actual')\n",
    "    constraint_bars = ax4.bar(x_pos, constraints, alpha=0.3, color='gray', label='Constraint')\n",
    "    \n",
    "    ax4.set_ylabel('Percentage (%)')\n",
    "    ax4.set_title('Performance vs Constraints')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (actual, constraint) in enumerate(zip(values, constraints)):\n",
    "        ax4.text(i, actual + max(values)*0.05, f'{actual:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        status = '✓' if actual <= constraint else '✗'\n",
    "        ax4.text(i, constraint + max(values)*0.05, status, ha='center', va='bottom', \n",
    "                fontsize=14, color='green' if status == '✓' else 'red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "=============================================================================\n",
    "SUMMARY REPORT\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRAUD DETECTION SYSTEM - FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_thresholds:\n",
    "    print(f\"\"\"\n",
    "SYSTEM CONFIGURATION:\n",
    "• Low Risk Threshold: {low_thresh:.3f}\n",
    "• High Risk Threshold: {high_thresh:.3f}\n",
    "• Model Ensemble: Logistic Regression (40%) + Random Forest (40%) + Isolation Forest (20%)\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "• Decline Rate: {best_results['decline_rate']*100:.2f}% (Target: ≤ 30%) ✓\n",
    "• Agent Alert Rate: {best_results['agent_alert_rate']*100:.4f}% (Target: < 0.1%) ✓\n",
    "• Missed Fraud Rate: {best_results['missed_fraud_rate']*100:.2f}% (Target: ≤ 2%) ✓\n",
    "\n",
    "TRANSACTION PROCESSING:\n",
    "• Low Risk (Auto-Approve): {best_results['low_risk_pct']:.1f}% of transactions\n",
    "• Medium Risk (Customer Validation): {best_results['medium_risk_pct']:.1f}% of transactions  \n",
    "• High Risk (Agent Review): {best_results['high_risk_pct']:.1f}% of transactions\n",
    "\n",
    "FRAUD DETECTION EFFECTIVENESS:\n",
    "• Total Frauds Detected: {best_results['medium_risk_frauds'] + best_results['high_risk_frauds']} out of {best_results['total_frauds']}\n",
    "• Detection Rate: {((best_results['medium_risk_frauds'] + best_results['high_risk_frauds']) / best_results['total_frauds'] * 100):.1f}%\n",
    "• Frauds Requiring Customer Validation: {best_results['medium_risk_frauds']}\n",
    "• Frauds Requiring Agent Review: {best_results['high_risk_frauds']}\n",
    "\n",
    "BUSINESS IMPACT:\n",
    "• Reduced manual review workload by processing {best_results['low_risk_pct']:.1f}% of transactions automatically\n",
    "• Efficient resource allocation with only {best_results['high_risk_pct']:.1f}% requiring agent attention\n",
    "• Customer friction minimized while maintaining strong fraud protection\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\\nRECOMMENDations FOR PRODUCTION DEPLOYMENT:\")\n",
    "    print(\"• Implement real-time risk scoring with the provided function\")\n",
    "    print(\"• Set up monitoring dashboards for the three key performance metrics\")\n",
    "    print(\"• Establish feedback loops to retrain models with new fraud patterns\")\n",
    "    print(\"• Create escalation procedures for high-risk transactions\")\n",
    "    print(\"• Regularly validate thresholds against business constraints\")\n",
    "    print(\"• Consider A/B testing before full deployment\")\n",
    "\n",
    "else:\n",
    "    print(\"SYSTEM OPTIMIZATION FAILED\")\n",
    "    print(\"Unable to find thresholds that meet all business constraints.\")\n",
    "    print(\"Recommended next steps:\")\n",
    "    print(\"• Improve feature engineering\")\n",
    "    print(\"• Collect more training data\")\n",
    "    print(\"• Consider relaxing business constraints\")\n",
    "    print(\"• Explore advanced modeling techniques\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis completed successfully!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.111943,
   "end_time": "2025-09-12T21:18:10.269754",
   "environment_variables": {},
   "exception": null,
   "input_path": "Fraud_notebook.ipynb",
   "output_path": "Fraud_notebook.executed.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T21:18:08.157811",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}